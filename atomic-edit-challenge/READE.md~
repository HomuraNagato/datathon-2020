
# Introduction

At the datathon event hosted by Brown Data Science during the weekend of February 22, 2020,
a dataset offered for analysis was wikiedits, gathered by Professor Pavlick of the Lunar lab
at Brown. This dataset included edits made on wikipedia including three variables, the original phrase, either an insertion or deletion phrase, and the edited phrase. The goal was to find something interesting about the types of edits made.

One possible analysis is to cluster the edits into groups to see if there are commonalities
among the sort of edits made. This is the goal of this project, begun during the datathon
and completed afterwards.

There are three major steps towards reaching this goal
 - vectorize the text into an embedding of shape [n_samples, embedding_size]
 - cluster the embedding vector into k-cluster [n_samples, k]
 - compress k-clusters down to two dimensions using t_sne [n_samples, 2]

Once this is done, a scatterplot can be produced to visualize the clusters.


# Vectorize text into embedding

Use sklearn's TfidVectorizer to convert a list of strings to an embedding vector.

TfidVectorizer first learns a vocabulary from input text = embedding_size. The formula to
calculate a word's tf-idf is below (reformated in probability terms):

<!-- perhaps this comment will work? -->

[comment]: <> idf(t|d) = log(\frac{1 + n}{1 + df(t)}) + 1

<a href="https://www.codecogs.com/eqnedit.php?latex=idf(t|d)&space;=&space;log(\frac{1&space;&plus;&space;n}{1&space;&plus;&space;df(t)})&space;&plus;&space;1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?idf(t|d)&space;=&space;log(\frac{1&space;&plus;&space;n}{1&space;&plus;&space;df(t)})&space;&plus;&space;1" title="idf(t|d) = log(\frac{1 + n}{1 + df(t)}) + 1" /></a>

[comment]: # tf-idf(t|d) = df(t|d) * idf(t|d)

<a href="https://www.codecogs.com/eqnedit.php?latex=tf-idf(t|d)&space;=&space;df(t|d)&space;*&space;idf(t|d)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?tf-idf(t|d)&space;=&space;df(t|d)&space;*&space;idf(t|d)" title="tf-idf(t|d) = df(t|d) * idf(t|d)" /></a>

$df(t)$ is the document frequency of the token $t$, the number of documents that include the token $t$; the more documents with $t$, the larger the value this will be.

$df(t|d)$ is the document frequency of the token $t$ given the document $d$, the number of occurences $t$ occurs in $d$; the more $t$ occurs in $d$, the larger the value will be.

$idf(t|d)$ is the inverse document frequency of the token $t$ in the document $d$, $n$ is the number of documents; the more a token is included in more documents, the lower this value will be.

$tf-idf(t|d)$ is the output value of the token given a specific document; the more a token occurs in a single document and less in every other document will produce a larger value and similarly a token that occurs infrequently in a particular document, but occurs frequently in all documents will have a small value.

In sum, this weights additional value to unique tokens in specific documents.

This produces something similar to a word embedding, converting a list of text into a matrix
of word vectors of shape [n_samples, vocabulary_size=embedding_size].


# cluster word embedding using k-means

Now that we vectorized our text, we wish to cluster. k-means transforms a vector of shape
[n_samples, embedding_size] to [n_samples, k], where $k$ are the number of clusters. The
formula used by sklearn is below

\[
 sum_{i=0}^{n} {min_{\mu_j \in C} (||x_i - \mu_j||^2)}
\]

This formula attempts to separate samples by inertia, or the within-cluster sum-of-squares. The
algorithm computes centroids with a stopping value when the computed centroids move less than
some tolerance. However in higher dimensions this minimization algorithm tends to perform poorly.
To accomodate this, first run PCA to reduce dimensions to some value larger than the number of clusters,
and smaller than the number of dimensions.